{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jpn-Eng-Translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2pKs8mrf9s9rsG6qT003C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Torey-Clark/japn-eng-translator/blob/local/Jpn_Eng_Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9itSt4Rjx8HL"
      },
      "source": [
        "<h1>Japanese to English Translations</h1>\n",
        "<h4>External Sources</h4>\n",
        "\n",
        "*   <a href=\"https://pypi.org/project/fugashi/\">Fugashi - Japanese Tokenizer</a>\n",
        "*   <a href=\"https://pypi.org/project/mecab-python3/\">MeCab - Analysis Engine</a>\n",
        "*   <a href=\"https://pypi.org/project/unidic/\">UniDic - Japanese Dictionary</a>\n",
        "*   <a href=\"http://www.manythings.org/bilingual/jpn/\">ManyThings - English-Japanese Sentence Pairs</a>\n",
        "\n",
        "<h4>Difficulties</h4>\n",
        "<p>For translating between English and Japanese, we need to identify the different words in Japanese sentences. Since Japanese does not use punctuation or spaces to signify divisions of words, we are using the tokenizer Fugashi, which is a reskin of MeCab, to identify different words and conjugations using the dictionary UniDic.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fcDsru-0r2P"
      },
      "source": [
        "<p>Install the libraries that are not native for Tensor Flow.\n",
        "\n",
        "Import each library we will be using throughout the system.\n",
        "\n",
        "Declare our tokenizing object from fugashi. For performance, it is far better to have a single instance of the tokenizer than to declare one for each sentence we want to tokenize.\n",
        "\n",
        "Set the path to the English-Japanese sentence pairs we have copied in GitHub.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVBXvuuhiP05",
        "outputId": "91533564-f197-4080-e47c-4a59acef25b9"
      },
      "source": [
        "!pip install fugashi\n",
        "!pip install mecab-python3\n",
        "!pip install fugashi[unidic]\n",
        "#!python -m unidic download #Only need to download the dictionary once. Can be commented out afterwards.\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import fugashi\n",
        "import MeCab\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import io\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialization\n",
        "jpn_tokenizer = fugashi.Tagger(\"-Owakati\")\n",
        "\n",
        "# Download File\n",
        "path_to_file = tf.keras.utils.get_file(\"jpn.txt\", \n",
        "                                       origin=\"https://raw.githubusercontent.com/Torey-Clark/japn-eng-translator/main/jpn.txt\", \n",
        "                                       extract=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: fugashi[unidic] in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: unidic; extra == \"unidic\" in /usr/local/lib/python3.6/dist-packages (from fugashi[unidic]) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from unidic; extra == \"unidic\"->fugashi[unidic]) (0.8.0)\n",
            "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.6/dist-packages (from unidic; extra == \"unidic\"->fugashi[unidic]) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /usr/local/lib/python3.6/dist-packages (from unidic; extra == \"unidic\"->fugashi[unidic]) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /usr/local/lib/python3.6/dist-packages (from unidic; extra == \"unidic\"->fugashi[unidic]) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.22.0->unidic; extra == \"unidic\"->fugashi[unidic]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.22.0->unidic; extra == \"unidic\"->fugashi[unidic]) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.22.0->unidic; extra == \"unidic\"->fugashi[unidic]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.22.0->unidic; extra == \"unidic\"->fugashi[unidic]) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "typ_jyzB1k_V"
      },
      "source": [
        "<p>\n",
        "Declare our collection of untility functions. Most of these function come directly from the tensorflow tutorial for translating between Spanish and English. Changes have been made were appropriate for our chosen languages.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJajm6VnkhF1"
      },
      "source": [
        "# Utility Functions - BEGIN\n",
        "debug = False\n",
        "# Convert unicode file to ascii\n",
        "# Not used since Japanese characters are only properly displayed using Unicode. We will print each sentence in Unicode for human readability.\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# Convert all sentences into our standard format.\n",
        "def preprocess_sentence_eng(sentence):\n",
        "    #sentence = unicode_to_ascii(sentence.lower().strip())\n",
        "\n",
        "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "    sentence.rstrip().strip()\n",
        "\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    if debug:\n",
        "        print(len(sentence))\n",
        "        print(sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "# Use Fugashi to tokenize Japanese sentences and place into our standard format.\n",
        "def preprocess_sentence_jpn(sentence):\n",
        "    # For the sake of human readability, we do not convert Japanese into ASCII.\n",
        "    #sentence = unicode_to_ascii(sentence.lower().strip())\n",
        "\n",
        "    words = [word.surface for word in jpn_tokenizer(sentence)]\n",
        "\n",
        "    sentence = '<start> '\n",
        "    for word in words:\n",
        "        sentence = sentence + word + ' '\n",
        "\n",
        "    #sentence = sentence[:-1] # Remove the trailing space.\n",
        "    sentence = sentence + '<end>'\n",
        "    if debug:\n",
        "        print(\"First word: \" + words[1])\n",
        "        print(\"Length of sentence: \" + len(sentence))\n",
        "        print(\"Sentence: \" + sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, JAPANESE]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = []\n",
        "    # For each line in the file, split it between English and Japanese\n",
        "    for l in lines:\n",
        "        temp_word_pairs = []\n",
        "        words = l.split('\\t')\n",
        "        temp_word_pairs.append(preprocess_sentence_eng(words[0])) # Pre-Process English sentences\n",
        "        temp_word_pairs.append(preprocess_sentence_jpn(words[1])) # Pre-Process Japanese sentences\n",
        "        word_pairs.append(temp_word_pairs)\n",
        "\n",
        "    print(\"Dataset Created\")\n",
        "    return zip(*word_pairs)\n",
        "\n",
        "def tokenize(lang): \n",
        "    print(\"Tokenizing\")\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    print(\"Tokenized\")\n",
        "    return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset(path, num_examples=None):\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            print(\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "# Utility Function - END"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCMMhOVl4L9D"
      },
      "source": [
        "<p>We can see that fugashi is properly tokenizing the Japanese sentence so we can use the pairs for training the model and translating future sentences.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt3yBu__kzyY",
        "outputId": "06c466d4-e3be-47bc-e086-f2c2283e47a5"
      },
      "source": [
        "# Example sentences\n",
        "eng_sentence_example = u\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\"\n",
        "jpn_sentence_example = u\"çãç«ã¡ãç¥ããªãäººã«ãã¤ãã£ãã¿ããã«èããããã£ã¦è¨ãããããããã¯ãããããããªãã®åãæ¹ã®ã©ãããããã¤ãã£ããããªãã¨æããããã£ã¦ãã¨ã ããã¤ã¾ãã­ããã¤ãã£ãã®ããã«ã¯èãããªãã¨ãããã¨ãªãã ãã\"\n",
        "\n",
        "print(preprocess_sentence_eng(eng_sentence_example))\n",
        "print(preprocess_sentence_jpn(jpn_sentence_example))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> If someone who doesn't know your background says that you sound like a native speaker , it means they probably noticed something about your speaking that made them realize you weren't a native speaker . In other words , you don't really sound like a native speaker .  <end>\n",
            "<start> çãç«ã¡ ã ç¥ã ãªã äºº ã« ãã¤ãã£ã ã¿ãã ã« èããã ã ã£ã¦ è¨ã ã ãã ã ãã ã¯ ãããã ã ããªã ã® åã æ¹ ã® ã©ã ã ã ã ãã¤ãã£ã ãã ãªã ã¨ æã ãã ã ã£ã¦ ãã¨ ã  ã ã ã¤ã¾ã ã­ ã ãã¤ãã£ã ã® ãã ã« ã¯ èãã ãªã ã¨ ãã ãã¨ ãª ã ã  ã ã <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJHbX3nooL6u",
        "outputId": "0492e6f0-f29f-4d28-fc34-501949034212"
      },
      "source": [
        "maximum_dataset_size = 250\n",
        "eng, jpn = create_dataset(path_to_file, None)\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, maximum_dataset_size)\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Created\n",
            "Dataset Created\n",
            "Tokenizing\n",
            "Tokenized\n",
            "Tokenizing\n",
            "Tokenized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mPR3MTe4ijv"
      },
      "source": [
        "<p>We declare how large the training set will be. A larger training set will take longer but will produce more accurate results.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqQ3FDPgpBDJ",
        "outputId": "d631a6e2-7971-4791-b502-21a4ce3e2e99"
      },
      "source": [
        "num_epochs = 1\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index) + 1\n",
        "vocab_tar_size = len(targ_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_taget_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_taget_batch.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 61]), TensorShape([64, 50]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWKS1jQupVl8"
      },
      "source": [
        "# Encoder\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True,  return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNJDj0JTpiug"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyqNChtcpbUT"
      },
      "source": [
        "# Attention\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUQgYLuUpm5N"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KmPWpZtpgha"
      },
      "source": [
        "# Decoder\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000oVD4pptFq",
        "outputId": "63e34d7c-e4f0-45c1-f9ff-114279da1104"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 10070)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5IuKc5Yp0hB"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# Function definitions\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUCsJa45qEfK"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK-gtRMuqN3W"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        #Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbRO1aMI42dY"
      },
      "source": [
        "<h3>Training</>\n",
        "<p>This is where the model is trained using the dataset we have and using the size we declared earlier.</>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-zlgV8TqUJz",
        "outputId": "d86c44b3-db62-4734-f9d4-09b502f990f2"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "            #test\n",
        "\n",
        "            # saving (checkpoint) the model every 2 epochs\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "            print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "            print('Time taken for 1 epoch {:.2f} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.6246\n",
            "Epoch 1 Loss 0.0000\n",
            "Time taken for 1 epoch 101.21 sec\n",
            "\n",
            "Epoch 1 Batch 100 Loss 0.8395\n",
            "Epoch 1 Loss 0.0022\n",
            "Time taken for 1 epoch 3452.83 sec\n",
            "\n",
            "Epoch 1 Batch 200 Loss 0.8238\n",
            "Epoch 1 Loss 0.0042\n",
            "Time taken for 1 epoch 6796.55 sec\n",
            "\n",
            "Epoch 1 Batch 300 Loss 0.7528\n",
            "Epoch 1 Loss 0.0059\n",
            "Time taken for 1 epoch 10138.24 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj7yJIcjqYNh"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    # Tokenize the Japanese sentence to have spaces between words\n",
        "    sentence = preprocess_sentence_jpn(sentence)\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zFYFZNKqeIX"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w2lX-yaqaYu"
      },
      "source": [
        "def translate(sentence):\n",
        "    print(\"Input sentence : \" + sentence)\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    #plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT5QxYqZqp-6"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REEQoz9eqgiM"
      },
      "source": [
        "# Restore Checkpoint\n",
        "print(\"Restoring latest checkpoint...\")\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "print(\"Checkpoint restored.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX49MZnDqi5K"
      },
      "source": [
        "eval_sentence = \"ãã¯ãããããã¾ã\"\n",
        "translate(eval_sentence)\n",
        "print(\"Actual Translation: Good morning.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}